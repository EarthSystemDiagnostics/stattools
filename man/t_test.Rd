% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/stattest-functions.R
\name{t_test}
\alias{t_test}
\title{Student's t-Test for autocorrelated data}
\source{
\code{stats:::t.test.default}
}
\usage{
t_test(
  x,
  y = NULL,
  a1.x = 0,
  a1.y = 0,
  alternative = c("two.sided", "less", "greater"),
  mu = 0,
  paired = FALSE,
  var.equal = FALSE,
  conf.level = 0.95,
  ...
)
}
\arguments{
\item{x}{a (non-empty) numeric vector of data values.}

\item{y}{an optional (non-empty) numeric vector of data values.}

\item{a1.x}{autocorrelation parameter at lag one of the data in \code{x}.}

\item{a1.y}{autocorrelation parameter at lag one of the data in \code{y}.}

\item{alternative}{a character string specifying the alternative hypothesis,
must be one of \code{"two.sided"} (default), \code{"greater"} or
\code{"less"}.  You can specify just the initial letter.}

\item{mu}{a number indicating the true value of the mean (or difference in
means if you are performing a two sample test).}

\item{paired}{a logical indicating whether you want a paired t-test.}

\item{var.equal}{a logical variable indicating whether to treat the two
variances as being equal. If ‘TRUE’ then the pooled variance is used to
estimate the variance otherwise the Welch (or Satterthwaite) approximation
to the degrees of freedom is used.}

\item{conf.level}{confidence level of the interval.}

\item{...}{further arguments to be passed to or from methods.}
}
\value{
see \code{?t.test}.
}
\description{
Perform a one-sample or a two-sample t-test accounting for autocorrelation of
the input data. This function is a modified version of the base R's default
\code{\link[stats]{t.test}}, which treats autocorrelation of the input data
by resetting the number of observations to the effective degrees of freedom
according to user-supplied estimates of the data's autocorrelation parameters
at lag one. Note that this code is adapted from the default method of the
t.test function, thus it cannot be used with class \code{formula}.
}
\examples{

# Check the performance of the test on autocorrelated data

# This function Monte Carlo samples autocorrelated data series from an AR1
# process and runs the default t test as well as the stattools t_test to
# estimate the tests' rejection rates, i.e. the relative number of false
# positives where the tests reject the null hypothesis
estimateRejectionRate <- function(nobs, nr, a1.x, a1.y = a1.x,
                                  method = c("one.sample", "two.sample")) {

 method <- match.arg(method)

 # wrapper for random generation of AR1 series
 redNoise <- function(a1, n) {c(arima.sim(list(ar = a1), n))}

 # MC sampling of p values
 pval <- array(dim = c(2, nr))
 for (i in 1 : nr) {

   x <- redNoise(a1.x, nobs)
   if (method == "two.sample") y <- redNoise(a1.y, nobs)

   if (method == "one.sample") {

     tmp1 <- t.test(x)
     tmp2 <- t_test(x, a1.x = a1.x)

   } else {

     tmp1 <- t.test(x, y)
     tmp2 <- t_test(x, y, a1.x = a1.x, a1.y = a1.y)

   }

   # count false positives
   if (tmp1$p.value <= 0.05) pval[1, i] <- 1 else pval[1, i] <- 0
   if (tmp2$p.value <= 0.05) pval[2, i] <- 1 else pval[2, i] <- 0

 }

 # return null hypothesis rejection rates (type I error)
 c(t.test.default = sum(pval[1, ]) / nr,
   t_test = sum(pval[2, ]) / nr)

}

# The expectation is a rejection rate of around 0.05; in the following
# examples we see that the default t.test shows higher than expected
# rejection rates for autocorrelated data while the t_test remedies this
# issue.

estimateRejectionRate(1000, 1000, 0.5)
estimateRejectionRate(1000, 1000, 0.9)
estimateRejectionRate(1000, 1000, 0.5, method = "two.sample")
estimateRejectionRate(1000, 1000, 0.5, 0.7, method = "two.sample")
estimateRejectionRate(1000, 1000, 0.2, 0.9, method = "two.sample")

}
\seealso{
\code{\link[stats]{t.test}}, \code{\link{getEffectiveDOF}}
}
\author{
R Core Team, Thomas Münch
}
